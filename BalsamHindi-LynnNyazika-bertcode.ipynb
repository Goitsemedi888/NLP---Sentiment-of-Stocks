{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE1LGysTD6J9"
      },
      "outputs": [],
      "source": [
        "# **Team 1**\n",
        "# **Balsam Hindi**\n",
        "# **Lynn Nyazika**\n",
        "# **Course:** AI 574 - Natural Language Processing (FALL I, 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Cryptocurrency markets are notoriously difficult to predict. Even the most experienced investors can have trouble anticipating market behavior. The volatile and complex nature of cryptocurrencies makes them notoriously difficult to model. However, we believe that deep learning models such as convolutional neural networks (CNNs) can potentially provide more accurate predictions. In our project, we will be customizing a model built on a pre-trained CNN model and Transformers to build a models that predict the sentiment of text and relates that to the  behavior of cryptocurrency markets. \n",
        "\n",
        "The objective of our project is to design a model that utilizes AI to predict the behavior of cryptocurrency markets, in this context specifically Bitcoin; based on the sentiment of crowds on social media, in this case specifically Twitter. Our suite of tools will be trained on a dataset of past social media data and market data. Once trained, it will be able to automatically detect market trends and make predictions accordingly. Ideally our model will have the potential to surpass previous results and provide accurate predictions of Bitcoin market behavior.\n",
        "\n",
        "Keywords: Bitcoin, market, twitter, social media, \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# **Data Collection**\n",
        "\n",
        "\n",
        "# **Bitcoin Tweets**\n",
        "# **https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets**\n",
        "# **Generate sentiment analysis model for Bitcoin-specific Twitter colloquialism.**\n",
        "# **Twitter Sentiment Dataset**\n",
        "# **https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset**\n",
        "# **Generate sentiment analysis model for more general Twitter colloquialism.**\n",
        "# **Sarcasm on Reddit**\n",
        "# **https://www.kaggle.com/datasets/danofer/sarcasm**\n",
        "# **Generate sarcasm detection model for social media context.**\n",
        "# **Bitcoin Historical Dataset**\n",
        "# **https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd**\n",
        "# **Utilized for bitcoin performance by day comparison against sentiment.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9njfKGBEBLI"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s2Uvtz2r-q"
      },
      "source": [
        "**Code to check gpu print our must be '/device:GPU:0' is enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG7RzxY2PNEN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahs8wNbO6AzN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pandas-datareader\n",
        "#!pip install yfinance --upgrade --no-cache-dir\n",
        "!pip install yfinance\n",
        "!pip install fix_yahoo_finance\n",
        "!pip3 install snscrape\n",
        "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
        "!pip install --upgrade pandas-datareader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve-fB1l72-Zb"
      },
      "source": [
        "**Generic Libraries for Data Processing and Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaBMoDx7EKw7"
      },
      "outputs": [],
      "source": [
        "#Libraries needed for Data Exploration\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "pd.__version__\n",
        "\n",
        "import numpy as np\n",
        "np.__version__\n",
        "\n",
        "import datetime\n",
        "import pandas_datareader.data as web\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go #ref https://plotly.com/python/candlestick-charts/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcX8Nrc9p2v"
      },
      "source": [
        "# **Libraries for the heavy duty stuff. Huggingface libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H-OjZ70FaHU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo0o52I0GKiv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "#from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Jg4ZDl8YEn"
      },
      "source": [
        "# **Please uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktdefTMx2lK4"
      },
      "outputs": [],
      "source": [
        "#Takes 45minutes to scrape historical data\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "#tweets_list = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "#for i,tweet in enumerate(sntwitter.TwitterSearchScraper('BTC-USD OR BITCOIN USD, since:2022-07-01 until:2022-08-01').get_items()):\n",
        "    #if i>100000:\n",
        "       # break\n",
        "    #tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n",
        "    \n",
        "# Creating a dataframe from the tweets list above\n",
        "#tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OQJjqZp8VFF"
      },
      "source": [
        "**Please uncomment to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkUaPEdy6eNg"
      },
      "outputs": [],
      "source": [
        "#Export file to local system save as csv.\n",
        "\n",
        "#tweets_df.to_csv(\"uncleaned_tweets.csv\", encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRlFlpwVEZPT"
      },
      "outputs": [],
      "source": [
        "#import file from local machine set delimeter and check for NAN's\n",
        "tweets_bitcoin = pd.read_csv('/content/drive/MyDrive/Datasets/uncleaned_tweets (1).csv', lineterminator='\\n', na_values=\"?\")# parse_dates=[\"Datetime\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CISDoURjEyIH"
      },
      "outputs": [],
      "source": [
        "#Drop NAN in this file there are non but the practice is useful, check length\n",
        "tweets_bitcoin.dropna()\n",
        "len(tweets_bitcoin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3Y99x26n8g"
      },
      "source": [
        "Boiler plate code to clean data : \n",
        "Reference Code - https://github.com/PushTheEnvelopeAI/Twitter_Stock_Prediction/blob/main/preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk3pMccTWWDd"
      },
      "outputs": [],
      "source": [
        "#remove emojis\n",
        "tweets_bitcoin['Text'] = tweets_bitcoin['Text'].str.replace('[^A-Za-z0-9]', ' ', flags=re.UNICODE)\n",
        "\n",
        "#Clean Text\n",
        "\n",
        "def Preprocess_Tweets(data):\n",
        "\t\t\n",
        "\tdata['Text_Cleaned'] = data['Text'].str.lower()\n",
        "  \n",
        "\n",
        "\t## FIX HYPERLINKS\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'https?:\\/\\/.*[\\r\\n]*', ' ',regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'www.*[\\r\\n]*', ' ',regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('https', '', regex=False)\n",
        "\n",
        "\n",
        "\t## FIX INDIVIDUAL SYMBOLS \n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(': ', ' ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(', ', ' ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('. ', ' ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[;\\n~]', ' ', regex=True)\n",
        "\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(\"[]'â€¦*™|]\", '', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[[()!?\"]', '', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('_', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('w/', ' with ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('f/', ' for ', regex=False)\n",
        "\n",
        "\n",
        "\t## FIX EMOJIS\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':)', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':-)', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':(', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':-(', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('0_o', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(';)', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('=^.^=', '', regex=False)\n",
        "\n",
        "\n",
        "\t## FIX % SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('%', ' percent ', regex=False)\n",
        "\n",
        "\n",
        "\t## FIX & SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' & ', ' and ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('&amp', ' and ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('&gt', ' greater than ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('q&a', 'question and answer', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('&', ' and ', regex=False)\n",
        "\n",
        "\t## FIX USER TAGS AND HASTAGS\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('@[a-z0-9]+', '', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('#[a-z0-9]+', '', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('@', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('#', '', regex=False)\n",
        "\t   \n",
        "\t## FIX EMBEDDED COMMAS AND PERIODS    \n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z]),([a-z])', r'\\1 \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]),([0-9])', r'\\1\\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])[+]+', r'\\1 ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(',', '', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('u.s.', ' us ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('\\.{2,}', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])\\.([a-z])', r'\\1 \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('pdating', 'updating', regex=False) \n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])\\.', r'\\1 ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'\\.([a-z])', r' \\1', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' . ', ' ', regex=False)\n",
        "\t\t\n",
        "\t## FIX + SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'[+]([0-9])', r'positive \\1', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' + ', ' and ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('+ ', ' ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])[+]([a-z])', r'\\1 and \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('+', '', regex=False)\n",
        "\t\n",
        "\t## FIX - SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])[-]+([a-z])', r'\\1 \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z]) - ([a-z])', r'\\1 to \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]) -([0-9\\.])', r'\\1 to \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r' [-]([0-9])', r' negative \\1', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])-([0-9\\.])', r'\\1 to \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]) - ([0-9\\.])', r'\\1 to \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9a-z])-([0-9a-z])', r'\\1 \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[-]+[>]', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' [-]+ ', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('-', ' ', regex=False)\n",
        "\n",
        "\t## FIX $ SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[$][0-9\\.]', ' dollars ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('$', '', regex=False)\n",
        "\n",
        "\t## FIX = SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('=', ' equals ', regex=False)\n",
        "\n",
        "\t## FIX / SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('b/c', ' because ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('b/out', ' break out ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('b/o', ' break out ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('p/e', ' pe ratio ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' [/]+ ', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1/2 ', ' .5 ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1/4 ', ' .25 ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 3/4 ', ' .75 ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1/3 ', ' .3 ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 2/3 ', ' .6 ', regex=False)\n",
        "\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[/]{2,}', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([a-z])/([a-z])', r'\\1 and \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[0-9]+/[0-9]+/[0-9]+', '', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]{3,})/([0-9\\.]{2,})', r'\\1 to \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9]{2,})/([0-9\\.]{3,})', r'\\1 to \\2', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[a-z0-9]+/[a-z0-9]+', ' ', regex=True)\n",
        "\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('/', '', regex=False)\n",
        "\n",
        "\t## FIX < > SYMBOLS\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[<]+ ', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('<', ' less than ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' [>]+', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('>', ' greater than ', regex=False)\n",
        "\n",
        "\t## FIX : SYMBOL\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[0-9]+:[0-9]+am', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('[0-9]+:[0-9]', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(':', ' ', regex=False)\n",
        "\n",
        "\t#FIX UNITS CUSTOMIZED\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('mrkt', 'market', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' vol ', ' volume ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' ptrend', ' positive trend ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' ppl', ' people ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' pts', ' points ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' pt', ' point ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' l(ol){1,}', ' laugh ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('imho', ' in my opinion ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace('prev ', 'previous ', regex=True)\n",
        "\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 1q', ' first quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 2q', ' second quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 3q', ' third quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 4q', ' fourth quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q1', ' first quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q2', ' second quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q3', ' third quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' q4', ' fourth quarter ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' 10q ', ' form 10 ', regex=False)\n",
        "\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])million', r'\\1 million ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])mil', r'\\1 million ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' mil ', ' million ', regex=False)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])billion', r'\\1 billion ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])cents', r'\\1 cents ', regex=True)\n",
        "\t\t\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].replace(r'([0-9])([a-z])', r'\\1 \\2', regex=True)\n",
        "\n",
        "\t## FIX EXTRA SPACES AND ENDING PUNCTUATION\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.replace(' +', ' ', regex=True)\n",
        "\tdata['Text_Cleaned'] = data['Text_Cleaned'].str.strip(' .!?,)(:-')\n",
        "\n",
        "\n",
        "\treturn data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bl0Y29Jau1m"
      },
      "outputs": [],
      "source": [
        "tweets_bitcoin[\"Text_Cleaned\"] = tweets_bitcoin[\"Text\"]\n",
        "tweets_bitcoin.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaPgfXo9a5Sq"
      },
      "outputs": [],
      "source": [
        "cleaned_text = Preprocess_Tweets(tweets_bitcoin)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5bc6r5jbAq_"
      },
      "outputs": [],
      "source": [
        "cleaned_text['Text_Cleaned'] = cleaned_text['Text_Cleaned'].str.lower()\n",
        "cleaned_text.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvSxmCsWFBZ6"
      },
      "outputs": [],
      "source": [
        "#Check length of tweets and show graphical distriution\n",
        "tweets_bitcoin['len'] = tweets_bitcoin['Text_Cleaned'].str.len()\n",
        "tweets_bitcoin['len'].hist().set_xlabel(\"Tweet length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCtTPJZ3Frb1"
      },
      "outputs": [],
      "source": [
        "#set length of tweet to 500. tf restricted to 512\n",
        "tweets_btc_filtrd = tweets_bitcoin[tweets_bitcoin['len'] <= 500]\n",
        "tweets_btc_filtrd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpLdBaaxCSa2"
      },
      "outputs": [],
      "source": [
        "print(tweets_btc_filtrd.groupby(['Text_Cleaned'])['len'].transform('max'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oA5DV9Q_d-5"
      },
      "outputs": [],
      "source": [
        "#get length and row of longest length tweet. is < 500.\n",
        "col = \"len\"\n",
        "max_x = tweets_btc_filtrd.loc[tweets_btc_filtrd[col].idxmax()]\n",
        "print (\"Maximum value of column \", col, \" and its corresponding row values:\", max_x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhpNsuUdc5-4"
      },
      "outputs": [],
      "source": [
        "#drop columns not needed\n",
        "tweets_bitcoin = tweets_bitcoin.drop(columns=['Username','Text'], errors='ignore')\n",
        "tweets_bitcoin.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_E5yM7JAZyi"
      },
      "outputs": [],
      "source": [
        "#sample a portion of the reamaining texts\n",
        "BTC_Tweets = tweets_btc_filtrd.sample(90000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hefKEYjgDcCE"
      },
      "outputs": [],
      "source": [
        "BTC_Tweets['len'].hist().set_xlabel(\"Tweet length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqLx5KEZVRTi"
      },
      "outputs": [],
      "source": [
        "BTC_Tweets.tail(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFkUwlukpkpS"
      },
      "outputs": [],
      "source": [
        "# define datetimes for start and end dates\n",
        "start_date = '2020-8-1'\n",
        "end_date = '2022-8-1'\n",
        "# import stock data for given period between start and end date form yahoo finance\n",
        "data = web.DataReader(\"BTC-USD\", data_source='yahoo', start=start_date, end=end_date)\n",
        "# display returned dataframe header\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBHHwHGfN2EU"
      },
      "outputs": [],
      "source": [
        "data ['growth'] = np.where((data[\"Open\"] > data[\"Close\"]), \"negative\", \"positive\")\n",
        "data.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffmS8Q_Hor8P"
      },
      "outputs": [],
      "source": [
        "data['Volume'].plot(figsize=(15,5), title ='Bitcoin-USD Stock Volumes Purchased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaT5DAQrmhQC"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Candlestick(x=data.index,\n",
        "                open=data['Open'],\n",
        "                high=data['High'],\n",
        "                low=data['Low'],\n",
        "                close=data['Close'])])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEuHkFwl-BBE"
      },
      "source": [
        "Default set to run Twitter Roberta. Uncomment and Finbert and # comment Roberta to run other model both use same tokenizer as per literature guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ4zlBbwGPaL"
      },
      "outputs": [],
      "source": [
        "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
        "#model1 = f\"yiyanghkust/finbert-tone\"\n",
        "model1 = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmrP_rxU94p1"
      },
      "source": [
        "Pipeline construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AVxqfWZIBsu"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVw1pJ_Kkwz"
      },
      "outputs": [],
      "source": [
        "#Test example rpovided on website to make sure everything is still bueno!\n",
        "nlp(\"Covid cases are increasing fast!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAuA-WhOIiGq"
      },
      "outputs": [],
      "source": [
        "#Sample Test from scraped data set\n",
        "BTC_Tweets['Text_Cleaned'][67884]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnSPj7R5dQ5S"
      },
      "outputs": [],
      "source": [
        "#Tokenize looking good \n",
        "encoded_text = tokenizer(BTC_Tweets['Text_Cleaned'][67884])\n",
        "encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VADp1j3ZI6C1"
      },
      "outputs": [],
      "source": [
        "#Dry run test one sample from dataset\n",
        "nlp(BTC_Tweets['Text_Cleaned'][67884])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRL1RYaFIPBT"
      },
      "outputs": [],
      "source": [
        "#Iterate through entire dataset column text and unique id \"Tweet Id\"\n",
        "sent_results = {}\n",
        "count = 0\n",
        "for i, d in tqdm(BTC_Tweets.iterrows(), total=len(BTC_Tweets)):\n",
        "    try:\n",
        "        sent = nlp(d[\"Text\"])\n",
        "        sent_results[d[\"Tweet Id\"]] = sent\n",
        "        count += 1\n",
        "        if count == 50000:\n",
        "          break\n",
        "    except RuntimeError:\n",
        "        print(f'Failed to run {sent_results[d[\"Tweet Id\"]]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3khc318iRP_"
      },
      "outputs": [],
      "source": [
        "sent_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv9Yq4EwIKbw"
      },
      "source": [
        "\n",
        "Reference code used to make charts\n",
        "https://github.com/RobMulla/twitch-stream-projects/blob/main/051-stock-sentiment/stock-sentiment.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4VrpRPlO75U"
      },
      "outputs": [],
      "source": [
        "sent_df = pd.DataFrame(sent_results).T\n",
        "sent_df[\"label\"] = sent_df[0].apply(lambda x: x[\"label\"])\n",
        "sent_df[\"score\"] = sent_df[0].apply(lambda x: x[\"score\"])\n",
        "sent_df = sent_df.merge(\n",
        "    tweets_bitcoin.set_index(\"Tweet Id\"), left_index=True, right_index=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP9TpoMrgUKS"
      },
      "outputs": [],
      "source": [
        "sent_df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbKyy3TsM8Ga"
      },
      "outputs": [],
      "source": [
        "sent_df.groupby(\"label\")[\"score\"].plot(kind=\"hist\", bins=50)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8mRo_nNbmGo"
      },
      "outputs": [],
      "source": [
        "sent_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I66RjIuhYuqi"
      },
      "outputs": [],
      "source": [
        "sent_df['Date'] = pd.to_datetime(sent_df.Datetime, format='%Y-%m-%d')\n",
        "sent_df['Date'] = sent_df[\"Date\"].dt.date\n",
        "sent_df =sent_df.set_index(['Date'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfNAZ8a3jXYi"
      },
      "outputs": [],
      "source": [
        "#sent_df.set_index('Datetime').groupby([pd.Grouper(freq='D'),'score']).mean()\n",
        "#sent_df.groupby([\"Date\"]).agg({\"score\": [pd.Series.mode, \"mean\"]})\n",
        "#mode = lambda x: x.mode() if len(x) > 2 else np.array(x)\n",
        "#sent_df.groupby('Date')['score'].agg(mode)\n",
        "sent_daily = sent_df.groupby('Date')['score'].agg(lambda x: pd.Series.mode(x)[0])\n",
        "sent_daily.head(50)\n",
        "len(sent_daily)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmbUki9MN0iP"
      },
      "outputs": [],
      "source": [
        "sent_daily.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeIgJPqLqx2T"
      },
      "outputs": [],
      "source": [
        "sent_and_stock = sent_daily.to_frame(\"sentiment\").merge(\n",
        "    data, left_index=True, right_index=True\n",
        ")\n",
        "\n",
        "ax = sent_and_stock[\"sentiment\"].plot(legend=\"Sentiment\")\n",
        "ax2 = ax.twinx()\n",
        "sent_and_stock[\"Volume\"].plot(ax=ax2, color=\"orange\", legend=\"Closing Price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UiVqxSuJZZk"
      },
      "outputs": [],
      "source": [
        "sent_and_stock.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMLo3nr4HZrG"
      },
      "outputs": [],
      "source": [
        "#Export file to local system save as csv.\n",
        "\n",
        "sent_df.to_csv(\"Roberta_tweets_reslabld.csv\", encoding='utf-8', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
